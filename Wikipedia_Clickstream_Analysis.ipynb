{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deedee-ke/pyspark-exploring_wikipedia_clickstreams/blob/main/Wikipedia_Clickstream_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b84805",
      "metadata": {
        "id": "d8b84805"
      },
      "source": [
        "# Analyzing Wikipedia Clickstream Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fddc058",
      "metadata": {
        "id": "8fddc058"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqUEsJ2OXDXP",
        "outputId": "01410c85-f627-4773-dae8-5e9046be0ba5"
      },
      "id": "UqUEsJ2OXDXP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=1bd7a5913fc927a8b3fc97f748979b4d8fd42ad3fa0a60fd03ba23a1ae3ef2cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33467f42",
      "metadata": {
        "id": "33467f42"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2313603",
      "metadata": {
        "id": "b2313603"
      },
      "source": [
        "## Task Group 1 - Introduction to Clickstream Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291ad84f",
      "metadata": {
        "id": "291ad84f"
      },
      "source": [
        "### Task 1\n",
        "Create a new `SparkSession` and assign it to a variable named `spark`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13837f16",
      "metadata": {
        "id": "13837f16"
      },
      "outputs": [],
      "source": [
        "# Create a new SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "        .appName(\"Wikipedia Clickstream Analysis\") \\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377fbbef",
      "metadata": {
        "id": "377fbbef"
      },
      "source": [
        "### Task 2\n",
        "\n",
        "Create an RDD from a list of sample clickstream counts and save it as `clickstream_counts_rdd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b1a38b",
      "metadata": {
        "id": "92b1a38b"
      },
      "outputs": [],
      "source": [
        "# Sample clickstream counts\n",
        "sample_clickstream_counts = [\n",
        "    [\"other-search\", \"Hanging_Gardens_of_Babylon\", \"external\", 47000],\n",
        "    [\"other-empty\", \"Hanging_Gardens_of_Babylon\", \"external\", 34600],\n",
        "    [\"Wonders_of_the_World\", \"Hanging_Gardens_of_Babylon\", \"link\", 14000],\n",
        "    [\"Babylon\", \"Hanging_Gardens_of_Babylon\", \"link\", 2500]\n",
        "]\n",
        "\n",
        "# Create RDD from sample data\n",
        "clickstream_counts_rdd =  spark.sparkContext.parallelize(sample_clickstream_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae6f0d2",
      "metadata": {
        "id": "4ae6f0d2"
      },
      "source": [
        "### Task 3\n",
        "\n",
        "Using the RDD from the previous step, create a DataFrame named `clickstream_sample_df`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12dd3ee7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12dd3ee7",
        "outputId": "db88c122-de44-49c5-95f3-dbd822ab8cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-------------+----------+\n",
            "|         source_page|         target_page|link_category|link_count|\n",
            "+--------------------+--------------------+-------------+----------+\n",
            "|        other-search|Hanging_Gardens_o...|     external|     47000|\n",
            "|         other-empty|Hanging_Gardens_o...|     external|     34600|\n",
            "|Wonders_of_the_World|Hanging_Gardens_o...|         link|     14000|\n",
            "|             Babylon|Hanging_Gardens_o...|         link|      2500|\n",
            "+--------------------+--------------------+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame from the RDD of sample clickstream counts\n",
        "columns = [\"source_page\", \"target_page\", \"link_category\", \"link_count\"]\n",
        "clickstream_sample_df = clickstream_counts_rdd.toDF(columns)\n",
        "\n",
        "# Display the DataFrame to the notebook\n",
        "clickstream_sample_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1268c0b0",
      "metadata": {
        "id": "1268c0b0"
      },
      "source": [
        "## Task Group 2 - Inspecting Clickstream Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de43074",
      "metadata": {
        "id": "1de43074"
      },
      "source": [
        "### Task 4\n",
        "\n",
        "Read the files in `./cleaned/clickstream/` into a new Spark DataFrame named `clickstream` and display the first few rows of the DataFrame in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e284f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e284f2",
        "outputId": "7291334a-11c2-4767-a1a3-bd2f5767cc27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+-------------+-------------+-----------+\n",
            "|           referrer|            resource|link_category|language_code|click_count|\n",
            "+-------------------+--------------------+-------------+-------------+-----------+\n",
            "|   Daniel_Day-Lewis|      Phantom_Thread|         link|           en|      43190|\n",
            "|     other-internal|      Phantom_Thread|     external|           en|      21683|\n",
            "|        other-empty|      Phantom_Thread|     external|           en|     169532|\n",
            "|90th_Academy_Awards|      Phantom_Thread|         link|           en|      40449|\n",
            "|       other-search|      Phantom_Thread|     external|           en|     536940|\n",
            "|       other-search|Tara_Grinstead_mu...|     external|           en|      30041|\n",
            "|       other-search|      Yossi_Benayoun|     external|           en|      11045|\n",
            "|        other-empty|       Parthiv_Patel|     external|           en|      11481|\n",
            "|       other-search|       Parthiv_Patel|     external|           en|      34953|\n",
            "|        other-empty|   Cosimo_de'_Medici|     external|           en|      16418|\n",
            "|       other-search|   Cosimo_de'_Medici|     external|           en|      22190|\n",
            "|       other-search|University_of_Geo...|     external|           en|      29963|\n",
            "|        other-empty|University_of_Geo...|     external|           en|      17325|\n",
            "|       other-search|Carbon_monoxide_d...|     external|           en|      13617|\n",
            "|        other-empty|      Marissa_Ribisi|     external|           en|      18979|\n",
            "|             Shinee|Kim_Jong-hyun_(si...|         link|           en|      24433|\n",
            "|       other-search|Kim_Jong-hyun_(si...|     external|           en|     162466|\n",
            "|        other-empty|Kim_Jong-hyun_(si...|     external|           en|      60193|\n",
            "|        other-empty|         Hello_Kitty|     external|           en|      10674|\n",
            "|       other-search|         Hello_Kitty|     external|           en|      23726|\n",
            "+-------------------+--------------------+-------------+-------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the target directory (`./cleaned/clickstream/`) into a DataFrame (`clickstream`)\n",
        "clickstream = spark.read.option(\"header\", True) \\\n",
        "                        .option(\"delimiter\", \"\\t\") \\\n",
        "                        .option(\"inferSchema\", True) \\\n",
        "                        .csv(\"/content/sample_data/part-00000-58fb80d1-6fa0-45cd-a14d-1e6c0ce2f34c-c000.csv\")\n",
        "\n",
        "# Display the DataFrame to the notebook\n",
        "clickstream.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdeb6013",
      "metadata": {
        "id": "bdeb6013"
      },
      "source": [
        "### Task 5\n",
        "\n",
        "Print the schema of the DataFrame in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "934cc169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "934cc169",
        "outputId": "9d875c04-7f3e-4cd4-829c-c202dee13497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- referrer: string (nullable = true)\n",
            " |-- resource: string (nullable = true)\n",
            " |-- link_category: string (nullable = true)\n",
            " |-- language_code: string (nullable = true)\n",
            " |-- click_count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the schema of the `clickstream` DataFrame to the notebook\n",
        "clickstream.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f18366",
      "metadata": {
        "id": "02f18366"
      },
      "source": [
        "### Task 6\n",
        "\n",
        "Drop the `language_code` column from the DataFrame and display the new schema in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fa2a20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17fa2a20",
        "outputId": "12491607-43ad-4858-cff7-38be6ae014cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+-------------+-----------+\n",
            "|           referrer|            resource|link_category|click_count|\n",
            "+-------------------+--------------------+-------------+-----------+\n",
            "|   Daniel_Day-Lewis|      Phantom_Thread|         link|      43190|\n",
            "|     other-internal|      Phantom_Thread|     external|      21683|\n",
            "|        other-empty|      Phantom_Thread|     external|     169532|\n",
            "|90th_Academy_Awards|      Phantom_Thread|         link|      40449|\n",
            "|       other-search|      Phantom_Thread|     external|     536940|\n",
            "|       other-search|Tara_Grinstead_mu...|     external|      30041|\n",
            "|       other-search|      Yossi_Benayoun|     external|      11045|\n",
            "|        other-empty|       Parthiv_Patel|     external|      11481|\n",
            "|       other-search|       Parthiv_Patel|     external|      34953|\n",
            "|        other-empty|   Cosimo_de'_Medici|     external|      16418|\n",
            "|       other-search|   Cosimo_de'_Medici|     external|      22190|\n",
            "|       other-search|University_of_Geo...|     external|      29963|\n",
            "|        other-empty|University_of_Geo...|     external|      17325|\n",
            "|       other-search|Carbon_monoxide_d...|     external|      13617|\n",
            "|        other-empty|      Marissa_Ribisi|     external|      18979|\n",
            "|             Shinee|Kim_Jong-hyun_(si...|         link|      24433|\n",
            "|       other-search|Kim_Jong-hyun_(si...|     external|     162466|\n",
            "|        other-empty|Kim_Jong-hyun_(si...|     external|      60193|\n",
            "|        other-empty|         Hello_Kitty|     external|      10674|\n",
            "|       other-search|         Hello_Kitty|     external|      23726|\n",
            "+-------------------+--------------------+-------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- referrer: string (nullable = true)\n",
            " |-- resource: string (nullable = true)\n",
            " |-- link_category: string (nullable = true)\n",
            " |-- click_count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Drop target columns\n",
        "clickstream = clickstream.drop(\"language_code\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "clickstream.show()\n",
        "# Display the new schema in the notebook\n",
        "clickstream.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33ed64a3",
      "metadata": {
        "id": "33ed64a3"
      },
      "source": [
        "### Task 7\n",
        "\n",
        "Rename `referrer` and `resource` to `source_page` and `target_page`, respectively,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b75baed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b75baed",
        "outputId": "b480b5bb-d434-4819-bf23-5b4ec415867b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+-------------+-----------+\n",
            "|        source_page|         target_page|link_category|click_count|\n",
            "+-------------------+--------------------+-------------+-----------+\n",
            "|   Daniel_Day-Lewis|      Phantom_Thread|         link|      43190|\n",
            "|     other-internal|      Phantom_Thread|     external|      21683|\n",
            "|        other-empty|      Phantom_Thread|     external|     169532|\n",
            "|90th_Academy_Awards|      Phantom_Thread|         link|      40449|\n",
            "|       other-search|      Phantom_Thread|     external|     536940|\n",
            "|       other-search|Tara_Grinstead_mu...|     external|      30041|\n",
            "|       other-search|      Yossi_Benayoun|     external|      11045|\n",
            "|        other-empty|       Parthiv_Patel|     external|      11481|\n",
            "|       other-search|       Parthiv_Patel|     external|      34953|\n",
            "|        other-empty|   Cosimo_de'_Medici|     external|      16418|\n",
            "|       other-search|   Cosimo_de'_Medici|     external|      22190|\n",
            "|       other-search|University_of_Geo...|     external|      29963|\n",
            "|        other-empty|University_of_Geo...|     external|      17325|\n",
            "|       other-search|Carbon_monoxide_d...|     external|      13617|\n",
            "|        other-empty|      Marissa_Ribisi|     external|      18979|\n",
            "|             Shinee|Kim_Jong-hyun_(si...|         link|      24433|\n",
            "|       other-search|Kim_Jong-hyun_(si...|     external|     162466|\n",
            "|        other-empty|Kim_Jong-hyun_(si...|     external|      60193|\n",
            "|        other-empty|         Hello_Kitty|     external|      10674|\n",
            "|       other-search|         Hello_Kitty|     external|      23726|\n",
            "+-------------------+--------------------+-------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- source_page: string (nullable = true)\n",
            " |-- target_page: string (nullable = true)\n",
            " |-- link_category: string (nullable = true)\n",
            " |-- click_count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Rename `referrer` and `resource` to `source_page` and `target_page`\n",
        "clickstream = clickstream.withColumnRenamed(\"referrer\", \"source_page\") \\\n",
        "                         .withColumnRenamed(\"resource\", \"target_page\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "clickstream.show()\n",
        "# Display the new schema in the notebook\n",
        "clickstream.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572ce589",
      "metadata": {
        "id": "572ce589"
      },
      "source": [
        "## Task Group 3 - Querying Clickstream Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b8a031d",
      "metadata": {
        "id": "2b8a031d"
      },
      "source": [
        "### Task 8\n",
        "\n",
        "Add the `clickstream` DataFrame as a temporary view named `clickstream` to make the data queryable with `sparkSession.sql()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f017bdd8",
      "metadata": {
        "id": "f017bdd8"
      },
      "outputs": [],
      "source": [
        "# Create a temporary view in the metadata for this `SparkSession`\n",
        "clickstream.createOrReplaceTempView(\"clickstream\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd76c53",
      "metadata": {
        "id": "afd76c53"
      },
      "source": [
        "### Task 9\n",
        "\n",
        "Filter the dataset to entries with `Hanging_Gardens_of_Babylon` as the `target_page` and order the result by `click_count` using PySpark DataFrame methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c20a4de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c20a4de",
        "outputId": "056e324b-0347-4d58-9bc4-6f1db62919cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-------------+-----------+\n",
            "|         source_page|         target_page|link_category|click_count|\n",
            "+--------------------+--------------------+-------------+-----------+\n",
            "|        other-search|Hanging_Gardens_o...|     external|      47088|\n",
            "|         other-empty|Hanging_Gardens_o...|     external|      34619|\n",
            "|Wonders_of_the_World|Hanging_Gardens_o...|         link|      14668|\n",
            "|Seven_Wonders_of_...|Hanging_Gardens_o...|         link|      12296|\n",
            "+--------------------+--------------------+-------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter and sort the DataFrame using PySpark DataFrame methods\n",
        "filtered_df = clickstream.filter(clickstream.target_page == \"Hanging_Gardens_of_Babylon\") \\\n",
        "                         .orderBy(clickstream.click_count.desc())\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5426a56",
      "metadata": {
        "id": "f5426a56"
      },
      "source": [
        "### Task 10\n",
        "\n",
        "Perform the same analysis as the previous exercise using a SQL query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49bbec6",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a49bbec6",
        "outputId": "0ebd2371-ffd9-4bf8-89aa-1b6c1f2ec6b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-------------+-----------+\n",
            "|         source_page|         target_page|link_category|click_count|\n",
            "+--------------------+--------------------+-------------+-----------+\n",
            "|        other-search|Hanging_Gardens_o...|     external|      47088|\n",
            "|         other-empty|Hanging_Gardens_o...|     external|      34619|\n",
            "|Wonders_of_the_World|Hanging_Gardens_o...|         link|      14668|\n",
            "|Seven_Wonders_of_...|Hanging_Gardens_o...|         link|      12296|\n",
            "+--------------------+--------------------+-------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter and sort the DataFrame using SQL\n",
        "query = \"\"\"\n",
        "SELECT * FROM clickstream\n",
        "WHERE target_page = 'Hanging_Gardens_of_Babylon'\n",
        "ORDER BY click_count DESC\n",
        "\"\"\"\n",
        "result_df = spark.sql(query)\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b4d53e9",
      "metadata": {
        "id": "0b4d53e9"
      },
      "source": [
        "### Task 11\n",
        "\n",
        "Calculate the sum of `click_count` grouped by `link_category` using PySpark DataFrame methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38bac86c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38bac86c",
        "outputId": "74277b63-0229-4a7c-e93d-d24e9f6a65e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+\n",
            "|link_category|total_clicks|\n",
            "+-------------+------------+\n",
            "|         link|    97805811|\n",
            "|        other|     9338172|\n",
            "|     external|  3248677856|\n",
            "+-------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Aggregate the DataFrame using PySpark DataFrame Methods\n",
        "category_counts_df = clickstream.groupBy(\"link_category\").sum(\"click_count\") \\\n",
        "                                .withColumnRenamed(\"sum(click_count)\", \"total_clicks\")\n",
        "\n",
        "category_counts_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461eadc9",
      "metadata": {
        "id": "461eadc9"
      },
      "source": [
        "### Task 12\n",
        "\n",
        "Perform the same analysis as the previous exercise using a SQL query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "817ff99e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817ff99e",
        "outputId": "11d6c35e-0ca6-42f7-97e4-00a4116e4b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+\n",
            "|link_category|total_clicks|\n",
            "+-------------+------------+\n",
            "|         link|    97805811|\n",
            "|        other|     9338172|\n",
            "|     external|  3248677856|\n",
            "+-------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Aggregate the DataFrame using SQL\n",
        "query = \"\"\"\n",
        "SELECT link_category, SUM(click_count) AS total_clicks\n",
        "FROM clickstream\n",
        "GROUP BY link_category\n",
        "\"\"\"\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b756c1",
      "metadata": {
        "id": "c8b756c1"
      },
      "source": [
        "## Task Group 4 - Saving Results to Disk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79110ef5",
      "metadata": {
        "id": "79110ef5"
      },
      "source": [
        "### Task 13\n",
        "\n",
        "Let's create a new DataFrame named `internal_clickstream` that only contains article pairs where `link_category` is `link`. Use `filter()` to select rows to a specific condition and `select()` to choose which columns to return from the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e74d43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29e74d43",
        "outputId": "323a26f6-5f44-4ccd-bc30-cc2f8c49646e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----------+\n",
            "|         source_page|         target_page|click_count|\n",
            "+--------------------+--------------------+-----------+\n",
            "|    Daniel_Day-Lewis|      Phantom_Thread|      43190|\n",
            "| 90th_Academy_Awards|      Phantom_Thread|      40449|\n",
            "|              Shinee|Kim_Jong-hyun_(si...|      24433|\n",
            "|      Agnyaathavaasi|        Anu_Emmanuel|      15020|\n",
            "|      Naa_Peru_Surya|        Anu_Emmanuel|      12361|\n",
            "|        Mariah_Carey|         Nick_Cannon|      16214|\n",
            "|               Kesha|Rainbow_(Kesha_al...|      11448|\n",
            "|  David_Attenborough|   John_Attenborough|      11252|\n",
            "|            Boney_M.|       Bobby_Farrell|      14095|\n",
            "|The_End_of_the_F*...|      Jessica_Barden|     237279|\n",
            "|   Quentin_Tarantino|   The_Hateful_Eight|      12018|\n",
            "|Ready_Player_One_...|        Olivia_Cooke|      17468|\n",
            "| Royal_Rumble_(2018)|Kevin_Owens_and_S...|      11503|\n",
            "|     Macaulay_Culkin|         Brenda_Song|      20477|\n",
            "|      Altered_Carbon|Altered_Carbon_(T...|      23962|\n",
            "|            Lil_Pump|          Smokepurpp|      36736|\n",
            "|       Fifth_Harmony|      Camila_Cabello|      30959|\n",
            "|Havana_(Camila_Ca...|      Camila_Cabello|      12803|\n",
            "|    Jennifer_Aniston|        John_Aniston|      26498|\n",
            "|Kingsman:_The_Gol...|Kingsman:_The_Sec...|      11969|\n",
            "+--------------------+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- source_page: string (nullable = true)\n",
            " |-- target_page: string (nullable = true)\n",
            " |-- click_count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame named `internal_clickstream`\n",
        "internal_clickstream = clickstream.filter(clickstream.link_category == \"link\") \\\n",
        "                                  .select(\"source_page\", \"target_page\", \"click_count\")\n",
        "\n",
        "\n",
        "# Display the first few rows of the DataFrame in the notebook\n",
        "internal_clickstream.show()\n",
        "internal_clickstream.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f45f94",
      "metadata": {
        "id": "45f45f94"
      },
      "source": [
        "### Task 14\n",
        "\n",
        "Using `DataFrame.write.csv()`, save the `internal_clickstream` DataFrame as CSV files in a directory called `./results/article_to_article_csv/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c015ca",
      "metadata": {
        "id": "93c015ca"
      },
      "outputs": [],
      "source": [
        "# Save the `internal_clickstream` DataFrame to a series of CSV files\n",
        "internal_clickstream.write.csv(\"./results/article_to_article_csv/\", mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa754d6b",
      "metadata": {
        "id": "aa754d6b"
      },
      "source": [
        "### Task 15\n",
        "\n",
        "Using `DataFrame.write.parquet()`, save the `internal_clickstream` DataFrame as parquet files in a directory called `./results/article_to_article_pq/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d542098",
      "metadata": {
        "id": "3d542098"
      },
      "outputs": [],
      "source": [
        "# Save the `internal_clickstream` DataFrame to a series of parquet files\n",
        "internal_clickstream.write.parquet(\"./results/article_to_article_pq/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01cbf22c",
      "metadata": {
        "id": "01cbf22c"
      },
      "source": [
        "### Task 16\n",
        "\n",
        "Close the `SparkSession` and underlying `SparkContext`. What happens if you we call `clickstream.show()` after closing the `SparkSession`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b2c378",
      "metadata": {
        "id": "55b2c378"
      },
      "outputs": [],
      "source": [
        "# Stop the notebook's `SparkSession` and `SparkContext`\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b663c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0b663c73",
        "outputId": "2741ea89-c1ac-4bcf-f30b-37434b7e97a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o77.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9bc631489045>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The SparkSession and sparkContext are stopped; the following line will throw an error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclickstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o77.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "source": [
        "# The SparkSession and sparkContext are stopped; the following line will throw an error:\n",
        "clickstream.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4298348c",
      "metadata": {
        "id": "4298348c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}